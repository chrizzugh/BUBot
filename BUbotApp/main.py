# -*- coding: utf-8 -*-
"""BUBOT Bi  App

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14NfFl4R8M6lM047atxDMWTBNBJ7nGvXk
"""

# !pip install numpy==1.18.5

# !pip install tensorflow==2.2.0

#run this in colab only, no need to include inside the code for web
import nltk
# nltk.download('wordnet') #for first run
# nltk.download('omw-1.4') #for first run
# nltk.download('stopwords') #for first run
# nltk.download('punkt') #for first run

"""---- THIS IS WHERE THE CODE FOR WEB STARTS----

# ATTENTION CLASS
"""

# import tensorflow as tf
# import os
# from tensorflow.python.keras.layers import Layer
# from tensorflow.python.keras import backend as K

# class AttentionLayer(Layer):

#     import tensorflow as tf
#     import os
#     from tensorflow.python.keras.layers import Layer
#     from tensorflow.python.keras import backend as K

#     """
#     This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).
#     There are three sets of weights introduced W_a, U_a, and V_a
#     """

#     def __init__(self, **kwargs):
#         super(AttentionLayer, self).__init__(**kwargs)

#     def build(self, input_shape):
#         assert isinstance(input_shape, list)
#         # Create a trainable weight variable for this layer.

#         self.W_a = self.add_weight(name='W_a',
#                                    shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),
#                                    initializer='uniform',
#                                    trainable=True)
#         self.U_a = self.add_weight(name='U_a',
#                                    shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),
#                                    initializer='uniform',
#                                    trainable=True)
#         self.V_a = self.add_weight(name='V_a',
#                                    shape=tf.TensorShape((input_shape[0][2], 1)),
#                                    initializer='uniform',
#                                    trainable=True)

#         super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end

#     def call(self, inputs, verbose=False):
#         """
#         inputs: [encoder_output_sequence, decoder_output_sequence]
#         """
#         assert type(inputs) == list
#         encoder_out_seq, decoder_out_seq = inputs
#         if verbose:
#             print('encoder_out_seq>', encoder_out_seq.shape)
#             print('decoder_out_seq>', decoder_out_seq.shape)

#         def energy_step(inputs, states):
#             """ Step function for computing energy for a single decoder state
#             inputs: (batchsize * 1 * de_in_dim)
#             states: (batchsize * 1 * de_latent_dim)
#             """

#             assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
#             assert isinstance(states, list) or isinstance(states, tuple), assert_msg

#             """ Some parameters required for shaping tensors"""
#             en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]
#             de_hidden = inputs.shape[-1]

#             """ Computing S.Wa where S=[s0, s1, ..., si]"""
#             # <= batch size * en_seq_len * latent_dim
#             W_a_dot_s = K.dot(encoder_out_seq, self.W_a)

#             """ Computing hj.Ua """
#             U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim
#             if verbose:
#                 print('Ua.h>', U_a_dot_h.shape)

#             """ tanh(S.Wa + hj.Ua) """
#             # <= batch_size*en_seq_len, latent_dim
#             Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)
#             if verbose:
#                 print('Ws+Uh>', Ws_plus_Uh.shape)

#             """ softmax(va.tanh(S.Wa + hj.Ua)) """
#             # <= batch_size, en_seq_len
#             e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)
#             # <= batch_size, en_seq_len
#             e_i = K.softmax(e_i)

#             if verbose:
#                 print('ei>', e_i.shape)

#             return e_i, [e_i]

#         def context_step(inputs, states):
#             """ Step function for computing ci using ei """

#             assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
#             assert isinstance(states, list) or isinstance(states, tuple), assert_msg

#             # <= batch_size, hidden_size
#             c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)
#             if verbose:
#                 print('ci>', c_i.shape)
#             return c_i, [c_i]

#         fake_state_c = K.sum(encoder_out_seq, axis=1)
#         fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim

#         """ Computing energy outputs """
#         # e_outputs => (batch_size, de_seq_len, en_seq_len)
#         last_out, e_outputs, _ = K.rnn(
#             energy_step, decoder_out_seq, [fake_state_e],
#         )

#         """ Computing context vectors """
#         last_out, c_outputs, _ = K.rnn(
#             context_step, e_outputs, [fake_state_c],
#         )

#         return c_outputs, e_outputs

#     def compute_output_shape(self, input_shape):
#         """ Outputs produced by the layer """
#         return [
#             tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),
#             tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))
#         ]

# """#DATA_PREPROCESSING.py"""

# from nltk.corpus import stopwords
# from nltk.tokenize import WhitespaceTokenizer
# from nltk.stem import WordNetLemmatizer
# tk = WhitespaceTokenizer()
# import re
# import string


# def prep_ques(txt, abbrev):
    
#     #A. converting text to lowercase
#     txt = txt.lower()

#     #C. noise removing where unnecessary symbols will be removed
    

#     txt = re.sub(r"'ll", " will",txt)
#     txt = re.sub(r"'m", " am",txt)
#     txt = re.sub(r"'s", " is", txt)
#     txt = re.sub(r"\'ve", " have", txt)
#     txt = re.sub(r"\'re", " are", txt)
#     txt = re.sub(r"\'d", " had", txt)
#     txt = re.sub(r"won't", "would not", txt)
#     txt = re.sub(r"'t", " not", txt)
#     txt = txt.translate(str.maketrans('', '', string.punctuation))
    
#     lst = ''
#     for token in txt.split():
#         try:
#             temp = abbrev[token]
#         except:
#             temp = token
#         lst = lst + ' ' +temp
#     txt = lst
    
#     #B. undergoing tokenization or splitting the sentences into words
#     txt = tk.tokenize(txt)
    
#     #D. removing of stop words
#     txt = [word for word in txt if not word in stopwords.words()]

#     #E. stemming or getting the root words
#     #removes necessary character, might affect predictions
    
#     #F. lemmatization 
#     lemmatizer = WordNetLemmatizer()
#     txt = [lemmatizer.lemmatize(token, pos="v") for token in txt]

#     return txt

# """#JSON_files.py"""

# import json
# def access_files():

#     with open('D:/Documents/thesis/BUBot/BUbotApp/models/abbrev.json', 'r') as abbreviations:
#         abbrev = json.load(abbreviations)

#     with open('D:/Documents/thesis/BUBot/BUbotApp/models/version2_vocab1.json', 'r') as vocabulary:
#         vocab = json.load(vocabulary)


#     return abbrev, vocab

#     del(abbreviations, vocabulary)

# """#SETUP TRAINING"""

# from tensorflow.keras.models import Model
# from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Attention
# import tensorflow as tf
# from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.initializers import Constant
# SEQUENCE_SIZE = 70
# LSTM_units = 512
# import numpy as np

# """#predict.py"""

# def predict(USER_INPUT, enc_model, dec_model, abbrev, vocab, inv_vocab):    

#     USER_INPUT = prep_ques(USER_INPUT, abbrev)

#     user = []
#     for token in USER_INPUT: 
#         try:
#             user.append(vocab[token])
#         except:
#             user.append(vocab['UNK'])
#     user = [user]    

#     user = pad_sequences(user, SEQUENCE_SIZE, padding='post', truncating = 'post')

#     enc_op, stat = enc_model.predict( user )

#     empty_target_seq = np.zeros( ( 1 , 1) ) 

#     empty_target_seq[0, 0] = vocab['SOS']

#     decoded_translation = ''

#     while True:

#         dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )

#         attn_layer = AttentionLayer()
#         attn_op, attn_state = attn_layer([enc_op, dec_outputs])
#         decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])
#         decoder_concat_input = dec_dense(decoder_concat_input)


#         sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )


#         sampled_word = inv_vocab[sampled_word_index] + ' '


#         if sampled_word != 'EOS ':
#             decoded_translation += sampled_word  

#         if sampled_word == 'EOS ' or len(decoded_translation.split()) > 70:
#             break 

#         empty_target_seq = np.zeros( ( 1 , 1 ) )  
#         empty_target_seq[ 0 , 0 ] = sampled_word_index

#         stat = [h, c]  

#     return decoded_translation

# """#MAIN.py
# EVERYTHING IS INSIDE A FUNCTION
# """

if __name__ == "__main__":

#     abbrev, vocab = access_files()
#     VOCAB_SIZE = len(vocab)
#     inv_vocab = {index:word for word, index in vocab.items()}

#     embed = Embedding(VOCAB_SIZE, 
#                     100,
#                     input_length=SEQUENCE_SIZE,
#                     trainable=True)

#     #ENCODER LAYER
#     enc_inp = Input(shape=(SEQUENCE_SIZE,))
#     enc_embed = embed(enc_inp)
#     enc_lstm = Bidirectional(LSTM(LSTM_units, return_state=True, dropout=0.2, return_sequences = True))
#     encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)
#     state_h = Concatenate()([forward_h, backward_h])
#     state_c = Concatenate()([forward_c, backward_c])
#     enc_states = [state_h, state_c]

#     #DECODER LAYER
#     dec_inp = Input(shape=(SEQUENCE_SIZE, ))
#     dec_embed = embed(dec_inp)
#     dec_lstm = LSTM(LSTM_units*2, return_state=True, return_sequences=True, dropout=0.2)
#     output, _, _ = dec_lstm(dec_embed, initial_state=enc_states)

#     # ATTENTION LAYER
#     attn_layer = AttentionLayer()
#     attn_op, attn_state = attn_layer([encoder_outputs, output])
#     decoder_concat_input = Concatenate(axis=-1)([output, attn_op])

#     #DENSE LAYER
#     dec_dense = Dense(VOCAB_SIZE, activation='softmax')
#     final_output = dec_dense(decoder_concat_input)

#     model = Model([enc_inp, dec_inp], final_output)


#     #ACCESS SAVED MODEL AND SET THE WEIGHTS
#     savedmodel = tf.keras.models.load_model('D:/Documents/thesis/BUBot/BUbotApp/models/VanilaBUbot')
#     model.set_weights(savedmodel.get_weights())

#     #---------------------------INFERENCE-----------------------------------------------
#     enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])


#     decoder_state_input_h = tf.keras.layers.Input(shape=( LSTM_units * 2,))
#     decoder_state_input_c = tf.keras.layers.Input(shape=( LSTM_units * 2,))

#     decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]


#     decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)


#     decoder_states = [state_h, state_c]

#     dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],
#                                             [decoder_outputs] + decoder_states)

    #-------------------------END OF INFERENCE------------------------------------------

    print("TESTING BUBOT.... \nPlease type your question....\nType 'EXIT' to quit program.") 
    print("------------------------------------------------------------")


    while True: 

        """
            Get the user input, perform preprocessing
            and necessary operations to transform output into machine-readable input
        """
        USER_INPUT = input("USER : ")

        if USER_INPUT == 'EXIT':
            break


        response = predict(USER_INPUT, enc_model, dec_model, abbrev, vocab, inv_vocab)

        print("BUBOT : ", response)
        print("============================================================")