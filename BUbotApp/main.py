# -*- coding: utf-8 -*-
"""BUBOT Bi TRaining

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JFiTnNy0_PEzr1suwl9B1cxnH13LQT_-
"""

# !pip install numpy==1.18.5

# pip install tensorflow==2.2.0

# pip show numpy

# pip show tensorflow

"""
    !Attention Layer
"""

import tensorflow as tf
import os
from tensorflow.python.keras.layers import Layer
from tensorflow.python.keras import backend as K

class AttentionLayer(Layer):
    """
        This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).
        There are three sets of weights introduced W_a, U_a, and V_a
    """
    
    def __init__(self, **kwargs):
        super(AttentionLayer, self).__init__(**kwargs)

    def build(self, input_shape):
        assert isinstance(input_shape, list)
        # Create a trainable weight variable for this layer.

        self.W_a = self.add_weight(name='W_a',
                                    shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),
                                    initializer='uniform',
                                    trainable=True)
        self.U_a = self.add_weight(name='U_a',
                                    shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),
                                    initializer='uniform',
                                    trainable=True)
        self.V_a = self.add_weight(name='V_a',
                                    shape=tf.TensorShape((input_shape[0][2], 1)),
                                    initializer='uniform',
                                    trainable=True)

        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end

    def call(self, inputs, verbose=False):
        """
        inputs: [encoder_output_sequence, decoder_output_sequence]
        """
        assert type(inputs) == list
        encoder_out_seq, decoder_out_seq = inputs
        if verbose:
            print('encoder_out_seq>', encoder_out_seq.shape)
            print('decoder_out_seq>', decoder_out_seq.shape)

        def energy_step(inputs, states):
            """ Step function for computing energy for a single decoder state
            inputs: (batchsize * 1 * de_in_dim)
            states: (batchsize * 1 * de_latent_dim)
            """

            assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
            assert isinstance(states, list) or isinstance(states, tuple), assert_msg

            """ Some parameters required for shaping tensors"""
            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]
            de_hidden = inputs.shape[-1]

            """ Computing S.Wa where S=[s0, s1, ..., si]"""
            # <= batch size * en_seq_len * latent_dim
            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)

            """ Computing hj.Ua """
            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim
            if verbose:
                print('Ua.h>', U_a_dot_h.shape)

            """ tanh(S.Wa + hj.Ua) """
            # <= batch_size*en_seq_len, latent_dim
            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)
            if verbose:
                print('Ws+Uh>', Ws_plus_Uh.shape)

            """ softmax(va.tanh(S.Wa + hj.Ua)) """
            # <= batch_size, en_seq_len
            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)
            # <= batch_size, en_seq_len
            e_i = K.softmax(e_i)

            if verbose:
                print('ei>', e_i.shape)

            return e_i, [e_i]

        def context_step(inputs, states):
            """ Step function for computing ci using ei """

            assert_msg = "States must be an iterable. Got {} of type {}".format(states, type(states))
            assert isinstance(states, list) or isinstance(states, tuple), assert_msg

            # <= batch_size, hidden_size
            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)
            if verbose:
                print('ci>', c_i.shape)
            return c_i, [c_i]

        fake_state_c = K.sum(encoder_out_seq, axis=1)
        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim

        """ Computing energy outputs """
        # e_outputs => (batch_size, de_seq_len, en_seq_len)
        last_out, e_outputs, _ = K.rnn(
            energy_step, decoder_out_seq, [fake_state_e],
        )

        """ Computing context vectors """
        last_out, c_outputs, _ = K.rnn(
            context_step, e_outputs, [fake_state_c],
        )

        return c_outputs, e_outputs

    def compute_output_shape(self, input_shape):
        """ Outputs produced by the layer """
        return [
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),
            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))
        ]




""" 
    !Data Preprocessing
"""
from . import preprocessing
import nltk
#! uncomment if first run
# nltk.download('stopwords') #for first run
# nltk.download('punkt') #for first run
# nltk.download('wordnet') #for first run
# nltk.download('omw-1.4') #for first run

from nltk.corpus import stopwords
from nltk.tokenize import WhitespaceTokenizer
from nltk.stem import WordNetLemmatizer
tk = WhitespaceTokenizer()
import re
import string


# def prep_ques(txt, abbrev):
    
#     #A. converting text to lowercase
#     txt = txt.lower()
  
#     #C. noise removing where unnecessary symbols will be removed
    

#     txt = re.sub(r"'ll", " will",txt)
#     txt = re.sub(r"'m", " am",txt)
#     txt = re.sub(r"'s", " is", txt)
#     txt = re.sub(r"\'ve", " have", txt)
#     txt = re.sub(r"\'re", " are", txt)
#     txt = re.sub(r"\'d", " had", txt)
#     txt = re.sub(r"won't", "would not", txt)
#     txt = re.sub(r"'t", " not", txt)
#     txt = txt.translate(str.maketrans('', '', string.punctuation))
    
#     lst = ''
#     for token in txt.split():
#         try:
#             temp = abbrev[token]
#         except:
#             temp = token
#         lst = lst + ' ' +temp
#     txt = lst
    
#     #B. undergoing tokenization or splitting the sentences into words
#     txt = tk.tokenize(txt)
    
#     #D. removing of stop words
#     txt = [word for word in txt if not word in stopwords.words()]

#     #E. stemming or getting the root words
#     #removes necessary character, might affect predictions
    
#     #F. lemmatization 
#     lemmatizer = WordNetLemmatizer()
#     txt = [lemmatizer.lemmatize(token, pos="v") for token in txt]

#     return txt

def prep_ans(txt):
    #A. converting text to lowercase
    
    txt = txt.lower()

    
    #C. noise removing where unnecessary symbols will be removed

    txt = re.sub(r"'ll", " will",txt)
    txt = re.sub(r"'m", " am",txt)
    txt = re.sub(r"'s", " is", txt)
    txt = re.sub(r"\'ve", " have", txt)
    txt = re.sub(r"\'re", " are", txt)
    txt = re.sub(r"\'d", " had", txt)
    txt = re.sub(r"won't", "would not", txt)
    txt = re.sub(r"'t", " not", txt)
    #txt = txt.translate(str.maketrans('', '', string.punctuation))
    
    #B. undergoing tokenization or splitting the sentences into words
    txt = 'SOS ' + txt + ' EOS'
    txt = tk.tokenize(txt) 
    

    return txt



"""
    !HYPERPARAMETERS and PARAMETERS
"""

LSTM_units = 512
DROP_OUT = 0.2
BATCH_SIZE = 32
EPOCHS = 1000
SEQUENCE_SIZE = 0
VOCAB_SIZE = 0




"""
    !Processing inputs, outputs
"""

import json
import numpy as np    
"""  
    1. access and open the json file
"""
with open('D:/Documents/thesis/BUBot/BUbotApp/models/final_intents.json', 'r', encoding="utf8") as access_to_json:
    json_data = json.loads(access_to_json.read())

with open('D:/Documents/thesis/BUBot/BUbotApp/models/abbrev.json', 'r', encoding="utf8") as abbreviations:
    abbrev = json.loads(abbreviations.read())




"""  
2. acess json_data
"""
data = json_data['intents']



"""
3. separate the answers and questions in two different lists
"""
questions = []
answers = []
for d in data:
    if len(d['patterns']) > 1:
        for pattern in d['patterns']:
            lst = []
            lst.append(pattern)
            questions.append(lst)
            answers.append(d['responses'])
    else:
        questions.append(d['patterns'])
        answers.append(d['responses'])

#delete unnecessesary variables    
del(access_to_json, d, data, json_data, lst, pattern)          

#append the contents to another
answers.extend(questions)
questions.extend(answers[0:300])



"""
4.perform preprocessing on dataset
"""
final_questions = []
final_answers = []

for line in questions:
    text = (preprocessing.prep_ques(line[0], abbrev))
    final_questions.append(text)


for line in answers:
    text = prep_ans(line[0])
    final_answers.append(text)

#delete unnecessesary variables
del(answers, questions, line, text)

len(final_questions)




"""
    !MAIN
"""

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Attention
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint
import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.initializers import Constant





"""
!CREATE VOCABULARY
"""

def create_vocab(ques, ans):
    vocab = {}

    vocab['PAD'] = 0
    vocab['UNK'] = 1
    word_num = 2
    for line in ques:
        for word in line:
            if word not in vocab:
                vocab[word] = word_num
                word_num += 1
                
    for line in ans:
        for word in line:
            if word not in vocab:
                vocab[word] = word_num
                word_num += 1       
    return vocab

"""
    5. Create vocabulary 
    """

vocab = create_vocab(final_questions, final_answers)
#inverse answers dict
inv_vocab = {index:word for word, index in vocab.items()}

VOCAB_SIZE = len(vocab)   
#delete unnecessesary variables

"""
    6. Find the maximum length of the strings in answers and questions
    """
if len(max(final_answers, key=len)) >= len(max(final_questions, key=len)):   
    SEQUENCE_SIZE = len(max(final_answers, key=len))
else:
    SEQUENCE_SIZE = len(max(final_questions, key=len))
        
print(SEQUENCE_SIZE)

len(max(final_answers, key=len))

vocab_file = open("D:/Documents/thesis/BUBot/BUbotApp/models/vocab.json", "w")
json.dump(vocab, vocab_file)

len(vocab)

"""
    9. create encoder input and decoder inputs
    """
encoder_input = []
for line in final_questions:
    lst = []
    for word in line:
            lst.append(vocab[word])
    encoder_input.append(lst)

decoder_input = []
for line in final_answers:
    lst = []
    for word in line:
            lst.append(vocab[word])        
    decoder_input.append(lst)

#delete unnecessesary variables
del(line, lst, word)

encoder_input = pad_sequences(encoder_input, SEQUENCE_SIZE, padding='post')
decoder_input = pad_sequences(decoder_input, SEQUENCE_SIZE, padding='post')

decoder_final_output = []
for i in decoder_input:
    decoder_final_output.append(i[1:]) 

decoder_final_output = pad_sequences(decoder_final_output, SEQUENCE_SIZE, padding='post')


del(i)

from tensorflow.keras.utils import to_categorical
decoder_final_output = to_categorical(decoder_final_output, VOCAB_SIZE)

decoder_final_output[0]




"""
    !PreTrain Embeddings
"""

# def embed(questions, answers):
#     from gensim.models import Word2Vec
    
#     inp = questions + answers
#     filename = "embedded.txt"
#     dec = Word2Vec(inp, min_count=1)
#     model = Word2Vec(inp, window=5, min_count=1, workers=4)
#     dec.wv.save_word2vec_format(filename, binary = False)

#embed(final_questions, final_answers)

embedded_index = {}

with open('D:/Documents/thesis/BUBot/BUbotApp/models/embedded_100.txt', encoding ='utf8') as f:
    f = f.read().split('\n')

for i in range(1, len(f)-1):
    values = f[i].split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32' )   
    embedded_index[word] = coefs

#delete unnecessesary variables
del(i, f, values, word, coefs)  


embedding_matrix = np.zeros((len(vocab), 100))

for i in inv_vocab:
    vector = embedded_index.get(inv_vocab[i])
    if vector is not None:
        embedding_matrix[i] = vector

embedding_matrix.shape




"""
    !TRAINING SETUP
"""

#EMBEDDING LAYER
embed = Embedding(VOCAB_SIZE, 
                    100, 
                    embeddings_initializer = Constant(embedding_matrix),
                    input_length=SEQUENCE_SIZE,
                    trainable=True)

#ENCODER LAYER
enc_inp = Input(shape=(SEQUENCE_SIZE,))
enc_embed = embed(enc_inp)
enc_lstm = Bidirectional(LSTM(LSTM_units, return_state=True, dropout=DROP_OUT, return_sequences = True))
encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)
state_h = Concatenate()([forward_h, backward_h])
state_c = Concatenate()([forward_c, backward_c])
enc_states = [state_h, state_c]

#DECODER LAYER
dec_inp = Input(shape=(SEQUENCE_SIZE, ))
dec_embed = embed(dec_inp)
dec_lstm = LSTM(LSTM_units*2, return_state=True, return_sequences=True, dropout=DROP_OUT)
output, _, _ = dec_lstm(dec_embed, initial_state=enc_states)

# ATTENTION LAYER
attn_layer = AttentionLayer()
attn_op, attn_state = attn_layer([encoder_outputs, output])
decoder_concat_input = Concatenate(axis=-1)([output, attn_op])

#DENSE LAYER
dec_dense = Dense(VOCAB_SIZE, activation='softmax')
final_output = dec_dense(decoder_concat_input)

model = Model([enc_inp, dec_inp], final_output)

model.summary()

with open('D:/Documents/thesis/BUBot/BUbotApp/models/modelsummary_Version2.txt', 'w') as f:

    model.summary(print_fn=lambda x: f.write(x + '\n'))

import tensorflow as tf
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint

es = EarlyStopping(monitor='loss', mode = 'min',verbose=1, patience=75)

mc = ModelCheckpoint('best_model', monitor='accuracy', mode='max', verbose=1, save_best_only=True)

history = model.fit([encoder_input, decoder_input], decoder_final_output, epochs=EPOCHS, callbacks=[es, mc])

loss_train = history.history['loss']
acc = history.history['acc']   
import matplotlib.pyplot as plt

epochs = range(1,408)

plt.plot(epochs, loss_train, 'r', label='Training loss')
plt.plot(epochs, acc, 'b', label=' Accuracy')
plt.title('Training loss and Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss and Accuracy')
plt.legend()
plt.show()

plt.savefig('D:/Documents/thesis/BUBot/BUbotApp/models/version1.png')


#FOR TESTING

savedmodel = tf.keras.models.load_model('D:/Documents/thesis/BUBot/BUbotApp/models/VanilaBUbot')
model.set_weights(savedmodel.get_weights())





"""
    !ATTENTION INFERENCE
"""

enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])


decoder_state_input_h = tf.keras.layers.Input(shape=( LSTM_units * 2,))
decoder_state_input_c = tf.keras.layers.Input(shape=( LSTM_units * 2,))

decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]


decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)


decoder_states = [state_h, state_c]

dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],
                                    [decoder_outputs] + decoder_states)

# print("TESTING BUBOT.... \nPlease type your question....\nType 'EXIT' to quit program.") 
# print("------------------------------------------------------------")


# while True: 

#     """
#         Get the user input, perform preprocessing
#         and necessary operations to transform output into machine-readable input
#     """
#     USER_INPUT = input("USER : ")

#     if USER_INPUT == 'EXIT':
#         break

#     USER_INPUT = preprocessing.prep_ques(USER_INPUT, abbrev)

#     #USER_INPUT = [USER_INPUT]

#     user = []
#     for token in USER_INPUT: 
#         try:
#             user.append(vocab[token])
#         except:
#             user.append(vocab['UNK'])
#     user = [user]    

#     user = pad_sequences(user, SEQUENCE_SIZE, padding='post', truncating = 'post')

#     enc_op, stat = enc_model.predict( user )

#     empty_target_seq = np.zeros( ( 1 , 1) ) 

#     empty_target_seq[0, 0] = vocab['SOS']

#     decoded_translation = ''

#     while True:

#         dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )


#         attn_op, attn_state = attn_layer([enc_op, dec_outputs])
#         decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])
#         decoder_concat_input = dec_dense(decoder_concat_input)


#         sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )


#         sampled_word = inv_vocab[sampled_word_index] + ' '


#         if sampled_word != 'EOS ':
#             decoded_translation += sampled_word  

#         if sampled_word == 'EOS ' or len(decoded_translation.split()) > 68:
#             break 

#         empty_target_seq = np.zeros( ( 1 , 1 ) )  
#         empty_target_seq[ 0 , 0 ] = sampled_word_index

#         stat = [h, c]  

#     print("BUBOT : ", decoded_translation )
#     print("============================================================")

model.save('D:/Documents/thesis/BUBot/BUbotApp/models/VanilaBUbot')



"""
    !TESTING
"""

test_data = final_questions[ 510 : ]

# response = [ ]
response_text = []

for test in test_data:

    USER_INPUT = test

    user = []
    for token in USER_INPUT: 
        try:
            user.append(vocab[token])
        except:
            user.append(vocab['UNK'])
    user = [user]    

    user = pad_sequences(user, SEQUENCE_SIZE, padding='post', truncating = 'post')

    enc_op, stat = enc_model.predict( user )

    empty_target_seq = np.zeros( ( 1 , 1) ) 

    empty_target_seq[0, 0] = vocab['SOS']

    decoded_translation = ['SOS']
    #decoded_translation = [vocab['SOS']]
    while True:

        dec_outputs , h, c= dec_model.predict([ empty_target_seq] + stat )


        attn_op, attn_state = attn_layer([enc_op, dec_outputs])
        decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])
        decoder_concat_input = dec_dense(decoder_concat_input)


        sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )


        sampled_word = inv_vocab[sampled_word_index]

        if sampled_word != 'EOS':
            decoded_translation.append(sampled_word)

        if sampled_word == 'EOS' or len(decoded_translation) > SEQUENCE_SIZE:
            decoded_translation.append(sampled_word)
            break 

        # if inv_vocab[sampled_word_index] != 'EOS':
        #     decoded_translation.append(sampled_word_index)

        # if inv_vocab[sampled_word_index] == 'EOS' or len(decoded_translation) > SEQUENCE_SIZE:
        #     decoded_translation.append(sampled_word_index)
        #     break 

        empty_target_seq = np.zeros( ( 1 , 1 ) )  
        empty_target_seq[ 0 , 0 ] = sampled_word_index

        stat = [h, c]  

    #print("BUBOT : ", decoded_translation )
    response_text.append(decoded_translation)
    #response.append(decoded_translation)
    print("Please wait...")

# decoder_input = []
# for line in final_answers:
#     lst = []
#     for word in line:
#             lst.append(vocab[word])        
#     decoder_input.append(lst)

target_data = decoder_input[ 510 : ]
# target_data = final_answers[ 510 : ]

target_data = pad_sequences(target_data, SEQUENCE_SIZE, padding='post')
response = pad_sequences(response, SEQUENCE_SIZE, padding='post')

target_data.shape

scores = [ ]



from sklearn.metrics import f1_score
i = 0
for resp in response:
    x = f1_score(target_data[i], resp, average='weighted')
    i = i+1
    scores.append(x)

# y_true = [[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]]
# y_pred = [[0, 2, 1, 0, 0, 1], [0, 2, 1, 0, 0, 1]]
# x = f1_score(y_true, y_pred, average='weighted')

scores



res = sum(scores) / 90.0
print(res)